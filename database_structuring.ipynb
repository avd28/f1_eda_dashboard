{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store_csv(csv_file, db_file, table_name, primary_key=None, foreign_keys=None, dropna=True, fillna_value=None):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, performs basic data cleaning, and stores the cleaned data into an SQLite3 database.\n",
    "\n",
    "    Args:\n",
    "    csv_file (str): The path to the CSV file.\n",
    "    db_file (str): The path to the SQLite3 database file.\n",
    "    table_name (str): The name of the table where the data will be stored.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file,na_values='\\\\N')\n",
    "\n",
    "    # Replace '\\n' with NaN\n",
    "    df.replace(to_replace=r'\\\\n', value=np.nan, regex=True, inplace=True)\n",
    "\n",
    "    # Basic data cleaning operations\n",
    "    \n",
    "    if dropna:\n",
    "        df.dropna(inplace=True)  # Drop rows with any NaN values\n",
    "    elif fillna_value is not None:\n",
    "        df.fillna(fillna_value, inplace=True)  # Fill NaN values with the specified value\n",
    "\n",
    "\n",
    "\n",
    "    # Connect to the SQLite3 database \n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop the table if it already exists\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "    # Map Datatype in SQLite database from dataframe dtype\n",
    "    dtype_mapping = {\n",
    "        'object': 'TEXT',\n",
    "        'int64': 'INTEGER',\n",
    "        'float64': 'REAL',\n",
    "        'datetime64[ns]': 'TEXT'  \n",
    "    }\n",
    "\n",
    "    columns = df.columns\n",
    "    col_defs = []\n",
    "    \n",
    "    for col in columns:\n",
    "        col_type = dtype_mapping[str(df[col].dtype)]\n",
    "        col_defs.append(f\"{col} {col_type}\")   \n",
    "\n",
    "    if primary_key:\n",
    "        col_defs += f\", PRIMARY KEY ({primary_key})\"\n",
    "\n",
    "    if foreign_keys:\n",
    "        for col, ref in foreign_keys.items():\n",
    "            col_defs += f\", FOREIGN KEY ({col}) REFERENCES {ref}\"\n",
    "\n",
    "    create_table_sql = f\"CREATE TABLE {table_name} ({col_defs})\"\n",
    "    cursor.execute(create_table_sql)\n",
    "    \n",
    "    # Write the cleaned data to the specified table\n",
    "    \n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Cleaned data from {csv_file} has been successfully stored in {db_file} in the {table_name} table.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(db_file, table_name, primary_key, key_value):\n",
    "    \"\"\"\n",
    "    Reads data from the SQLite3 database and performs a simple SQL query using the primary key.\n",
    "\n",
    "    Args:\n",
    "    db_file (str): The path to the SQLite3 database file.\n",
    "    table_name (str): The name of the table to query.\n",
    "    primary_key (str): The column to use as the primary key for the query.\n",
    "    key_value (any): The value of the primary key to search for.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the query result.\n",
    "    \"\"\"\n",
    "    # Connect to the SQLite3 database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Perform the query\n",
    "    query = f\"SELECT * FROM {table_name} WHERE {primary_key} = ?\"\n",
    "    cursor.execute(query, (key_value,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Fetch the column names\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "\n",
    "    # Convert the result to a pandas DataFrame\n",
    "    df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_index_column_to_csv(csv_file, index_column_name, output_csv_file):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, adds an index column to the leftmost column with the supplied column name,\n",
    "    and writes the updated data back to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "    csv_file (str): The path to the input CSV file.\n",
    "    index_column_name (str): The name for the new index column.\n",
    "    output_csv_file (str): The path to the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Create the index column\n",
    "    df.insert(0, index_column_name, range(1, len(df) + 1))\n",
    "\n",
    "    # Write the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "    print(f\"Index column '{index_column_name}' has been added and written to {output_csv_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data from ./dataset/circuits.csv has been successfully stored in williams_f1_database.db in the circuits_table table.\n"
     ]
    }
   ],
   "source": [
    "csv_file = './dataset/circuits.csv'  \n",
    "db_file = 'f1_database.db'  \n",
    "table_name = 'circuits_table' \n",
    "primary_key = 'circuitId' \n",
    "clean_and_store_csv(csv_file, db_file, table_name, primary_key,dropna=False,fillna_value='N/A' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data from ./dataset/races.csv has been successfully stored in williams_f1_database.db in the races_table table.\n"
     ]
    }
   ],
   "source": [
    "csv_file = './dataset/races.csv' \n",
    "db_file = 'f1_database.db' \n",
    "table_name = 'races_table'\n",
    "primary_key = 'raceId' \n",
    "foreign_keys = {'circuitId': 'circuits_table(circuitId)'}\n",
    "clean_and_store_csv(csv_file, db_file, table_name, primary_key,dropna=False,fillna_value='N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = 'f1_database.db' \n",
    "table_name = 'races_table'  \n",
    "primary_key = 'raceId'   \n",
    "key_value = 1062        \n",
    "result_df = query_database(db_file, table_name, primary_key, key_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data from ./dataset/drivers.csv has been successfully stored in williams_f1_database.db in the drivers_table table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitv\\AppData\\Local\\Temp\\ipykernel_18100\\825136183.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NaN' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(fillna_value, inplace=True)  # Fill NaN values with the specified value\n"
     ]
    }
   ],
   "source": [
    "csv_file = './dataset/drivers.csv'  \n",
    "db_file = 'f1_database.db'    \n",
    "table_name = 'drivers_table' \n",
    "primary_key = 'driverId'\n",
    "clean_and_store_csv(csv_file, db_file, table_name, primary_key,dropna=False,fillna_value='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data from ./dataset/driver_standings.csv has been successfully stored in williams_f1_database.db in the driver_standing_table table.\n"
     ]
    }
   ],
   "source": [
    "csv_file = './dataset/driver_standings.csv'  \n",
    "db_file = 'f1_database.db'    \n",
    "table_name = 'driver_standing_table' \n",
    "primary_key = 'driverStandingsId' \n",
    "foreign_keys = {'raceId': 'races_table(raceId)', 'driverId': 'drivers_table(driverId)'}  # Example foreign key definition #Name of column for foreign key\n",
    "clean_and_store_csv(csv_file, db_file, table_name, primary_key,foreign_keys,dropna=False,fillna_value='N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index column 'laptimesId' has been added and written to ./dataset/lap_times.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csv_file = './dataset/lap_times.csv'  \n",
    "index_column_name = 'laptimesId'  \n",
    "output_csv_file = './dataset/lap_times.csv' \n",
    "\n",
    "add_index_column_to_csv(csv_file, index_column_name, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data from ./dataset/lap_times.csv has been successfully stored in williams_f1_database.db in the lap_times table.\n"
     ]
    }
   ],
   "source": [
    "csv_file = './dataset/lap_times.csv' \n",
    "db_file = 'f1_database.db'    \n",
    "table_name = 'lap_times' \n",
    "primary_key = 'laptimesId'\n",
    "foreign_keys = {'raceId': 'races_table(raceId)', 'driverId': 'drivers_table(driverId)'}  \n",
    "clean_and_store_csv(csv_file, db_file, table_name, primary_key,dropna=False,fillna_value='N/A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
